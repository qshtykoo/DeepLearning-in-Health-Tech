{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_model_compression.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9iBj8FpSsvbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5BRzberIxFDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "598bb5e9-11ae-47f1-e9e0-37aaa14823c3"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D , Activation\n",
        "from keras.models import model_from_json\n",
        "\n",
        "#manually setting the image_data_format\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "\n",
        "\n",
        "# from skimage import io\n",
        "# def show(img):\n",
        "\t# io.imshow(img)\n",
        "\t# io.show()\n",
        "\t\n",
        "def softmax_c(z):\n",
        "\tassert len(z.shape) == 2\n",
        "\ts = np.max(z, axis=1)\n",
        "\ts = s[:, np.newaxis]\n",
        "\te_x = np.exp(z - s)\n",
        "\tdiv = np.sum(e_x, axis=1)\n",
        "\tdiv = div[:, np.newaxis] \n",
        "\treturn e_x / div\n",
        "\n",
        "def prepare_softtargets(model,X):\n",
        "\tinp = model.input                                           # input placeholder\n",
        "\toutputs = []\n",
        "\tfor layer in model.layers[:]:\n",
        "\t\tif layer.name == 'flatten_1':\n",
        "\t\t\toutputs.append(layer.output)\n",
        "\t\tif layer.name == 'dense_2':\n",
        "\t\t\toutputs.append(layer.output)\n",
        "\t\t\t\n",
        "\tfunctor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
        "\tlayer_outs = functor([X, 1.])\n",
        "\treturn np.array(layer_outs[0]) , np.array(layer_outs[1])\n",
        "\n",
        "\n",
        "# Todo parse as cmd argments \n",
        "TRAIN_BIG = False  \n",
        "TRAIN_SMALL = False \n",
        "PRPARE_TRAININPUT = True \n",
        "PRPARE_TESTINPUT = True \n",
        "\n",
        "# big model \n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "BIG_epochs = 20\n",
        "\n",
        "# Small Model\n",
        "STUDENT_epochs = 30\n",
        "HiddenNeuron = 8   # found out after experimentation. \n",
        "\n",
        "\n",
        "### Setup Data \n",
        "#load cifar10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "#load model architecture\n",
        "with open(r'C10M.json', 'r') as f:\n",
        "            model = model_from_json(f.read())\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "\t\t\t  optimizer=keras.optimizers.Adam(lr=0.0001, decay=1e-6),\n",
        "\t\t\t  metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 30, 30)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 30, 30)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 15, 15)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32, 15, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 15, 15)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 64, 15, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 64, 13, 13)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64, 13, 13)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 64, 6, 6)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64, 6, 6)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8cmb9ykrxXQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#normalize the data\n",
        "x_train = x_train  / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4JbH2Gb7szGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2753
        },
        "outputId": "25c82659-5dc7-4fa7-9c03-e567e3d7f223"
      },
      "cell_type": "code",
      "source": [
        "print (\"loading weights TRAIN_BIG FLAG set as FALSE\")\n",
        "model.load_weights(r'C10M.h5')\n",
        "\n",
        "print (\"Evaluating Initial Model ...\\n\")\n",
        "score = model.evaluate(x_test , y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print ('-'*30)\n",
        "\n",
        "print (\"Parameter Size of Initial Model and Memory Footprint \")\n",
        "trainable_params = model.count_params()\n",
        "footprint = trainable_params * 4\n",
        "print (\"Memory footprint per Image Feed Forward ~= \" , footprint / 1024.0 /1024.0 ,\"Mb\") # 2x Backprop\n",
        "print ('-'*30)\n",
        "\n",
        "\n",
        "## Obtaining the Output of the last Convolutional Layer After Flatten. Caruana et. al \n",
        "## and Preparing the SoftTargets, (Logits), as proposed Geoffery Hinton et. al \n",
        "\n",
        "\n",
        "\n",
        "if PRPARE_TRAININPUT:\n",
        "\tprint (\"Creating trainsfer Set\")\n",
        "\n",
        "\tlastconv_out = []\n",
        "\tlogit_out = []\n",
        "\tfor i in range(0,50):\n",
        "\t\tprint (\"Batch # : \",i)\n",
        "\t\tl,l2 =  (prepare_softtargets(model,x_train[i*1000:(i+1)*1000]))\n",
        "\t\tlastconv_out.append(l)\n",
        "\t\tlogit_out.append(l2)\n",
        "\n",
        "\tlastconv_out = np.array(lastconv_out)\n",
        "\tlogit_out = np.array(logit_out)\n",
        "\tlastconv_out = lastconv_out.reshape((50000 , 2304))\n",
        "\tlogit_out = logit_out.reshape((50000 , 10))\n",
        "\n",
        "\tprint (\"clean up \")\n",
        "\tx_train = 0\n",
        "\tprint (\"Write to Disk\")\n",
        "\th5f = h5py.File('new_lastconv_out.h5', 'w')\n",
        "\th5f.create_dataset('dataset_1', data=lastconv_out)\n",
        "\th5f.close()\n",
        "\th5f2 = h5py.File('new_logit_out.h5', 'w')\n",
        "\th5f2.create_dataset('dataset_1', data=logit_out)\n",
        "\th5f2.close()\n",
        "\n",
        "else:\n",
        "\tprint (\"loading Transfer Set from lastconv_out.h5\")\n",
        "\th5f = h5py.File('lastconv_out.h5' , 'r')\n",
        "\tlastconv_out = h5f['dataset_1'][:]\n",
        "\th5f.close()\n",
        "\n",
        "\th5f2 = h5py.File('logit_out.h5' , 'r')\n",
        "\tlogit_out = h5f2['dataset_1'][:]\n",
        "\th5f2.close()\n",
        "\n",
        "\n",
        "\n",
        "print (\"Building minimal Model\")\n",
        "\n",
        "student_model = Sequential()\n",
        "student_model.add(Dense(HiddenNeuron,input_dim=2304,activation='relu'))\n",
        "student_model.add(Dropout(0.2))\n",
        "student_model.add(Dense(num_classes))\n",
        "\n",
        "student_model.compile(loss='mse',\n",
        "\t\t\t  optimizer=keras.optimizers.Adadelta(),\n",
        "\t\t\t  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "TRAIN_SMALL = True\n",
        "\n",
        "if TRAIN_SMALL: \n",
        "\tprint (\"Training Small Model \")\n",
        "\tstudent_model.fit(lastconv_out,logit_out,nb_epoch=STUDENT_epochs,verbose=1 , batch_size=batch_size)\n",
        "\tstudent_model.save_weights(\"new_student_weights_6_0.2dopout.h5\")\n",
        "else :\n",
        "\tprint (\"Loading Small Model Weights\")\n",
        "\tstudent_model.load_weights(\"student_weights_6_0.2dopout.h5\")\n",
        "\n",
        "\n",
        "\n",
        "print (\"Clean up small model Training and targets\")\n",
        "lastconv_out = 0\n",
        "logit_out = 0 \n",
        "\n",
        "\n",
        "############ Preparing Test Input #########\n",
        "\n",
        "if PRPARE_TESTINPUT:\n",
        "\tprint (\"creating Test data from the big Model on HeldOut data\")\n",
        "\t\n",
        "\ttest_lastconv_out = []\n",
        "\ttest_logit_out = []\n",
        "\tfor i in range(0,10):\n",
        "\t\tprint (\"Batch # : \",i)\n",
        "\t\tl,l2 =  prepare_softtargets(model,x_test[i*1000:(i+1)*1000])\n",
        "\t\ttest_lastconv_out.append(l)\n",
        "\t\ttest_logit_out.append(l2)    \n",
        "\t\n",
        "\t# lastconv_out.shape , logit_out.shape\n",
        "\ttest_lastconv_out = np.array(test_lastconv_out)\n",
        "\ttest_logit_out = np.array(test_logit_out)\n",
        "\n",
        "\ttest_lastconv_out = test_lastconv_out.reshape((10000 , 2304))\n",
        "\ttest_logit_out = test_logit_out.reshape((10000 , 10))\n",
        "\n",
        "\tprint (test_lastconv_out.shape)\n",
        "\tprint (test_logit_out.shape)\n",
        "\n",
        "\tprint (\"Write to Disk\")\n",
        "\th5f = h5py.File('new_test_lastconv_out.h5', 'w')\n",
        "\th5f.create_dataset('dataset_1', data=lastconv_out)\n",
        "\th5f.close()\n",
        "\th5f2 = h5py.File('new_test_logit_out.h5', 'w')\n",
        "\th5f2.create_dataset('dataset_1', data=logit_out)\n",
        "\th5f2.close()\n",
        "else :\n",
        "\tprint (\"Loading saved test data from .h5 . \")\n",
        "\th5f = h5py.File('test_lastconv_out.h5' , 'r')\n",
        "\ttest_lastconv_out = h5f['dataset_1'][:]\n",
        "\th5f.close()\n",
        "\th5f2 = h5py.File('test_logit_out.h5' , 'r')\n",
        "\ttest_logit_out = h5f2['dataset_1'][:]\n",
        "\th5f2.close()\n",
        "\t\n",
        "pred = student_model.predict(test_lastconv_out)\n",
        "probs = softmax_c(pred)\n",
        "pred_classes = np.argmax(probs,axis=1)\n",
        "\n",
        "accuracy_student = metrics.accuracy_score(y_pred=pred_classes,y_true=np.argmax(y_test,axis=1))\n",
        "print (\"Small Model Test Set Accuracy : \" , accuracy_student)\n",
        "print (\"\\n\")\n",
        "# Compression Rate from Number of Parameters Reduced\n",
        "\n",
        "# Parameters for the first two conv layers from Bigger model\n",
        "convparams = 320 + 18496\n",
        "\n",
        "print (\"Evaluating COompression ... \")\n",
        "print (\"HiddenNeurons : \" , HiddenNeuron)\n",
        "print (\"Initial Model Parameters : \" , model.count_params())\n",
        "print (\"Compressed Model parameters + initial feature extractor part params : \", student_model.count_params() + convparams)\n",
        "compressionRate = model.count_params() / np.float(student_model.count_params()  + convparams)\n",
        "print (\"Compression Rate : \" , compressionRate)\n",
        "print (\"\\n\\n\")\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\t# Set usage using flags. \n",
        "\t# python runme.py --TRAIN_BIG=True --epochs=20 "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading weights TRAIN_BIG FLAG set as FALSE\n",
            "Evaluating Initial Model ...\n",
            "\n",
            "10000/10000 [==============================] - 2s 174us/step\n",
            "Test loss: 0.6733700441360474\n",
            "Test accuracy: 0.7846\n",
            "------------------------------\n",
            "Parameter Size of Initial Model and Memory Footprint \n",
            "Memory footprint per Image Feed Forward ~=  4.771644592285156 Mb\n",
            "------------------------------\n",
            "Creating trainsfer Set\n",
            "Batch # :  0\n",
            "Batch # :  1\n",
            "Batch # :  2\n",
            "Batch # :  3\n",
            "Batch # :  4\n",
            "Batch # :  5\n",
            "Batch # :  6\n",
            "Batch # :  7\n",
            "Batch # :  8\n",
            "Batch # :  9\n",
            "Batch # :  10\n",
            "Batch # :  11\n",
            "Batch # :  12\n",
            "Batch # :  13\n",
            "Batch # :  14\n",
            "Batch # :  15\n",
            "Batch # :  16\n",
            "Batch # :  17\n",
            "Batch # :  18\n",
            "Batch # :  19\n",
            "Batch # :  20\n",
            "Batch # :  21\n",
            "Batch # :  22\n",
            "Batch # :  23\n",
            "Batch # :  24\n",
            "Batch # :  25\n",
            "Batch # :  26\n",
            "Batch # :  27\n",
            "Batch # :  28\n",
            "Batch # :  29\n",
            "Batch # :  30\n",
            "Batch # :  31\n",
            "Batch # :  32\n",
            "Batch # :  33\n",
            "Batch # :  34\n",
            "Batch # :  35\n",
            "Batch # :  36\n",
            "Batch # :  37\n",
            "Batch # :  38\n",
            "Batch # :  39\n",
            "Batch # :  40\n",
            "Batch # :  41\n",
            "Batch # :  42\n",
            "Batch # :  43\n",
            "Batch # :  44\n",
            "Batch # :  45\n",
            "Batch # :  46\n",
            "Batch # :  47\n",
            "Batch # :  48\n",
            "Batch # :  49\n",
            "clean up \n",
            "Write to Disk\n",
            "Building minimal Model\n",
            "Training Small Model \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "50000/50000 [==============================] - 3s 61us/step - loss: 24.2106 - acc: 0.3700\n",
            "Epoch 2/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 18.6041 - acc: 0.4733\n",
            "Epoch 3/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 17.7811 - acc: 0.5040\n",
            "Epoch 4/30\n",
            "50000/50000 [==============================] - 3s 50us/step - loss: 17.3465 - acc: 0.5170\n",
            "Epoch 5/30\n",
            "50000/50000 [==============================] - 3s 50us/step - loss: 16.9858 - acc: 0.5287\n",
            "Epoch 6/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.9164 - acc: 0.5338\n",
            "Epoch 7/30\n",
            "50000/50000 [==============================] - 3s 50us/step - loss: 16.6951 - acc: 0.5408\n",
            "Epoch 8/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.6523 - acc: 0.5433\n",
            "Epoch 9/30\n",
            "50000/50000 [==============================] - 3s 50us/step - loss: 16.5705 - acc: 0.5416\n",
            "Epoch 10/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.4863 - acc: 0.5430\n",
            "Epoch 11/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.4132 - acc: 0.5470\n",
            "Epoch 12/30\n",
            "50000/50000 [==============================] - 3s 51us/step - loss: 16.3497 - acc: 0.5471\n",
            "Epoch 13/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.3736 - acc: 0.5464\n",
            "Epoch 14/30\n",
            "50000/50000 [==============================] - 3s 50us/step - loss: 16.3087 - acc: 0.5488\n",
            "Epoch 15/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.3358 - acc: 0.5498\n",
            "Epoch 16/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.2843 - acc: 0.5476\n",
            "Epoch 17/30\n",
            "50000/50000 [==============================] - 3s 51us/step - loss: 16.2913 - acc: 0.5498\n",
            "Epoch 18/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.2798 - acc: 0.5482\n",
            "Epoch 19/30\n",
            "50000/50000 [==============================] - 3s 51us/step - loss: 16.1357 - acc: 0.5509\n",
            "Epoch 20/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.2018 - acc: 0.5520\n",
            "Epoch 21/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.2106 - acc: 0.5522\n",
            "Epoch 22/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.1746 - acc: 0.5518\n",
            "Epoch 23/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.2379 - acc: 0.5503\n",
            "Epoch 24/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.1868 - acc: 0.5516\n",
            "Epoch 25/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.2220 - acc: 0.5496\n",
            "Epoch 26/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.2505 - acc: 0.5485\n",
            "Epoch 27/30\n",
            "50000/50000 [==============================] - 2s 50us/step - loss: 16.2639 - acc: 0.5474\n",
            "Epoch 28/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.1584 - acc: 0.5512\n",
            "Epoch 29/30\n",
            "50000/50000 [==============================] - 2s 49us/step - loss: 16.1124 - acc: 0.5514\n",
            "Epoch 30/30\n",
            "50000/50000 [==============================] - 3s 51us/step - loss: 16.1799 - acc: 0.5509\n",
            "Clean up small model Training and targets\n",
            "creating Test data from the big Model on HeldOut data\n",
            "Batch # :  0\n",
            "Batch # :  1\n",
            "Batch # :  2\n",
            "Batch # :  3\n",
            "Batch # :  4\n",
            "Batch # :  5\n",
            "Batch # :  6\n",
            "Batch # :  7\n",
            "Batch # :  8\n",
            "Batch # :  9\n",
            "(10000, 2304)\n",
            "(10000, 10)\n",
            "Write to Disk\n",
            "Small Model Test Set Accuracy :  0.6231\n",
            "\n",
            "\n",
            "Evaluating COompression ... \n",
            "HiddenNeurons :  8\n",
            "Initial Model Parameters :  1250858\n",
            "Compressed Model parameters + initial feature extractor part params :  37346\n",
            "Compression Rate :  33.49376104535961\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cmTar9ZjRvEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = student_model.predict(lastconv_out)\n",
        "probs = softmax_c(pred)\n",
        "pred_classes = np.argmax(probs,axis=1)\n",
        "\n",
        "accuracy_student = metrics.accuracy_score(y_pred=pred_classes,y_true=np.argmax(y_test,axis=1))\n",
        "accuracy_student"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}